"""
Metrics Helper for CLI Scripts
==============================

This module provides a lightweight metrics interface for CLI scripts (rad_dataframe.py,
rad_chunk.py, etc.) that can work with or without the full Prometheus metrics stack.

When running in the FastAPI context, metrics are exported via /metrics endpoint.
When running as CLI scripts, metrics are logged and can optionally be pushed to
a Prometheus Pushgateway if configured.

Usage:
    from scripts.metrics_helper import track_pdf_extraction, track_embedding_generation

    # Track PDF extraction
    with track_pdf_extraction('mistral') as tracker:
        text = extract_pdf(...)
        tracker.set_success(True)

    # Or use decorators
    @track_duration('pdf_extraction', provider='mistral')
    def extract_pdf(...):
        ...
"""

import os
import time
import logging
from functools import wraps
from contextlib import contextmanager
from typing import Optional, Callable, Any, Dict

logger = logging.getLogger(__name__)

# Try to import Prometheus client
try:
    from prometheus_client import (
        Counter, Histogram, Gauge,
        CollectorRegistry, push_to_gateway, REGISTRY
    )
    PROMETHEUS_AVAILABLE = True
except ImportError:
    PROMETHEUS_AVAILABLE = False
    Counter = None
    Histogram = None
    Gauge = None

# Configuration
PUSHGATEWAY_URL = os.getenv('PROMETHEUS_PUSHGATEWAY_URL')
METRICS_ENABLED = os.getenv('ENABLE_METRICS', 'true').lower() in ('true', '1', 'yes')
METRICS_JOB_NAME = os.getenv('METRICS_JOB_NAME', 'ragpy_cli')


class MetricsCollector:
    """
    Centralized metrics collector that works in both CLI and web contexts.
    """

    def __init__(self):
        self._counters: Dict[str, int] = {}
        self._histograms: Dict[str, list] = {}
        self._initialized = False
        self._registry = None

        if PROMETHEUS_AVAILABLE and METRICS_ENABLED:
            self._init_prometheus_metrics()

    def _init_prometheus_metrics(self):
        """Initialize Prometheus metrics if available."""
        try:
            # Use the default registry for web context compatibility
            self._registry = REGISTRY

            # PDF processing metrics
            self.pdf_processed = Counter(
                'ragpy_cli_pdf_processed_total',
                'Total PDFs processed by CLI',
                ['provider', 'status'],
                registry=self._registry
            )

            self.pdf_duration = Histogram(
                'ragpy_cli_pdf_duration_seconds',
                'PDF extraction duration',
                ['provider'],
                buckets=[1, 5, 10, 30, 60, 120, 300],
                registry=self._registry
            )

            # Chunk processing metrics
            self.chunks_generated = Counter(
                'ragpy_cli_chunks_generated_total',
                'Total chunks generated by CLI',
                ['phase'],
                registry=self._registry
            )

            self.chunking_duration = Histogram(
                'ragpy_cli_chunking_duration_seconds',
                'Chunking duration per document',
                buckets=[0.1, 0.5, 1, 2, 5, 10, 30],
                registry=self._registry
            )

            # Embedding metrics
            self.embeddings_generated = Counter(
                'ragpy_cli_embeddings_generated_total',
                'Total embeddings generated by CLI',
                ['model', 'type'],
                registry=self._registry
            )

            self.embedding_duration = Histogram(
                'ragpy_cli_embedding_batch_duration_seconds',
                'Embedding batch duration',
                ['model'],
                buckets=[0.5, 1, 2, 5, 10, 20, 60],
                registry=self._registry
            )

            # Error tracking
            self.errors = Counter(
                'ragpy_cli_errors_total',
                'Total errors in CLI processing',
                ['operation', 'error_type'],
                registry=self._registry
            )

            self._initialized = True
            logger.debug("Prometheus metrics initialized for CLI")

        except Exception as e:
            logger.warning(f"Failed to initialize Prometheus metrics: {e}")
            self._initialized = False

    def increment_counter(self, name: str, value: int = 1, **labels):
        """Increment a counter metric."""
        if not METRICS_ENABLED:
            return

        # Track locally
        key = f"{name}_{labels}"
        self._counters[key] = self._counters.get(key, 0) + value

        # Track in Prometheus if available
        if self._initialized:
            try:
                metric = getattr(self, name, None)
                if metric and hasattr(metric, 'labels'):
                    metric.labels(**labels).inc(value)
            except Exception as e:
                logger.debug(f"Failed to increment Prometheus counter: {e}")

    def observe_histogram(self, name: str, value: float, **labels):
        """Observe a value in a histogram."""
        if not METRICS_ENABLED:
            return

        # Track locally
        key = f"{name}_{labels}"
        if key not in self._histograms:
            self._histograms[key] = []
        self._histograms[key].append(value)

        # Track in Prometheus if available
        if self._initialized:
            try:
                metric = getattr(self, name, None)
                if metric and hasattr(metric, 'labels'):
                    metric.labels(**labels).observe(value)
            except Exception as e:
                logger.debug(f"Failed to observe Prometheus histogram: {e}")

    def push_to_gateway(self):
        """Push metrics to Pushgateway if configured."""
        if not PUSHGATEWAY_URL or not self._initialized:
            return False

        try:
            push_to_gateway(
                PUSHGATEWAY_URL,
                job=METRICS_JOB_NAME,
                registry=self._registry
            )
            logger.info(f"Metrics pushed to {PUSHGATEWAY_URL}")
            return True
        except Exception as e:
            logger.warning(f"Failed to push metrics to gateway: {e}")
            return False

    def get_summary(self) -> Dict[str, Any]:
        """Get a summary of collected metrics."""
        summary = {
            'counters': dict(self._counters),
            'histograms': {}
        }

        for key, values in self._histograms.items():
            if values:
                summary['histograms'][key] = {
                    'count': len(values),
                    'sum': sum(values),
                    'avg': sum(values) / len(values),
                    'min': min(values),
                    'max': max(values)
                }

        return summary


# Global collector instance
_collector = MetricsCollector()


@contextmanager
def track_pdf_extraction(provider: str = 'unknown'):
    """
    Context manager for tracking PDF extraction metrics.

    Usage:
        with track_pdf_extraction('mistral') as tracker:
            text = extract_pdf(path)
            tracker.set_success(True)
    """
    start_time = time.time()
    success = False

    class Tracker:
        def set_success(self, value: bool):
            nonlocal success
            success = value

    tracker = Tracker()

    try:
        yield tracker
    finally:
        duration = time.time() - start_time
        status = 'success' if success else 'failure'

        _collector.increment_counter('pdf_processed', provider=provider, status=status)
        _collector.observe_histogram('pdf_duration', duration, provider=provider)

        if not success:
            _collector.increment_counter('errors', operation='pdf_extraction', error_type='extraction_failed')


@contextmanager
def track_chunking():
    """Context manager for tracking chunking metrics."""
    start_time = time.time()

    class Tracker:
        def __init__(self):
            self.chunk_count = 0

        def add_chunks(self, count: int):
            self.chunk_count += count

    tracker = Tracker()

    try:
        yield tracker
    finally:
        duration = time.time() - start_time
        _collector.observe_histogram('chunking_duration', duration)
        _collector.increment_counter('chunks_generated', tracker.chunk_count, phase='initial')


@contextmanager
def track_embedding_batch(model: str = 'text-embedding-3-large'):
    """Context manager for tracking embedding generation metrics."""
    start_time = time.time()

    class Tracker:
        def __init__(self):
            self.embedding_count = 0

        def add_embeddings(self, count: int):
            self.embedding_count += count

    tracker = Tracker()

    try:
        yield tracker
    finally:
        duration = time.time() - start_time
        _collector.observe_histogram('embedding_duration', duration, model=model)
        _collector.increment_counter('embeddings_generated', tracker.embedding_count, model=model, type='dense')


def track_error(operation: str, error_type: str):
    """Track an error occurrence."""
    _collector.increment_counter('errors', operation=operation, error_type=error_type)


def get_metrics_summary() -> Dict[str, Any]:
    """Get a summary of all collected metrics."""
    return _collector.get_summary()


def push_metrics():
    """Push metrics to Pushgateway if configured."""
    return _collector.push_to_gateway()


def log_metrics_summary():
    """Log a summary of collected metrics."""
    summary = get_metrics_summary()

    logger.info("=== Metrics Summary ===")
    for counter_key, value in summary['counters'].items():
        logger.info(f"  {counter_key}: {value}")

    for hist_key, stats in summary['histograms'].items():
        logger.info(f"  {hist_key}: count={stats['count']}, avg={stats['avg']:.2f}s, max={stats['max']:.2f}s")
